{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d0e124c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Augmentor\n",
      "  Obtaining dependency information for Augmentor from https://files.pythonhosted.org/packages/f3/86/5a91176650eb229ea2cd95551c34c36fba6cd95da3bdc4a5c73fbb1536ca/Augmentor-0.2.12-py2.py3-none-any.whl.metadata\n",
      "  Downloading Augmentor-0.2.12-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: Pillow>=5.2.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from Augmentor) (10.0.1)\n",
      "Requirement already satisfied: tqdm>=4.9.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from Augmentor) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from Augmentor) (1.24.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\nisha\\anaconda3\\lib\\site-packages (from tqdm>=4.9.0->Augmentor) (0.4.6)\n",
      "Downloading Augmentor-0.2.12-py2.py3-none-any.whl (38 kB)\n",
      "Installing collected packages: Augmentor\n",
      "Successfully installed Augmentor-0.2.12\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# First we need to install Augmentor\u001b[39;00m\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install Augmentor\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, models\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# First we need to install Augmentor\n",
    "!pip install Augmentor\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import Augmentor\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "# Connect Google account to access train and test folders\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define paths for the train and test images \n",
    "# Note for local run: Code locally consumes a lot of memory and time, using Google Colab helps. Not recommended to run locally. \n",
    "# Note for mount folder: Folder name changed from 'Skin cancer ISIC The International Skin Imaging Collaboration' to 'Cancer_Dataset' to meet Google Drive requirement.\n",
    "# Note for coding standards: Folders were renamed from 'Train' to 'train' and from 'Test' to 'test' to meet coding practices.\n",
    "train_dir = \"/content/drive/MyDrive/Cancer_Dataset/train/\"\n",
    "test_dir = \"/content/drive/MyDrive/Cancer_Dataset/test/\"\n",
    "\n",
    "# Dataset Creation with ImageDataGenerator\n",
    "# ImageDataGenerator is used to preprocess the images and create batches with data augmentation\n",
    "batch_size = 32  # Number of images to be processed in one go (batch size)\n",
    "img_height, img_width = 180, 180  # Resize all images to 180x180 pixels\n",
    "\n",
    "# Create an instance of ImageDataGenerator for training and validation, rescaling images to 0-1 range\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)  # Rescale pixel values between 0 and 1\n",
    "\n",
    "# Load training images from directory, split the data for training and validation (80% training, 20% validation)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse',  # Labels are integer encoded\n",
    "    subset='training'  # Training subset\n",
    ")\n",
    "\n",
    "# Load validation images from directory\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse',  # Labels are integer encoded\n",
    "    subset='validation'  # Validation subset\n",
    ")\n",
    "\n",
    "# Visualizing the Dataset\n",
    "# Display images from each class to understand how they look\n",
    "class_names = list(train_generator.class_indices.keys())  # Get class names (disease categories)\n",
    "\n",
    "# Plot 9 images from the dataset\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_generator:\n",
    "    for i in range(9):\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        plt.imshow(images[i])  # Show the image\n",
    "        plt.title(class_names[int(labels[i])])  # Display the class name\n",
    "        plt.axis('off')  # Hide the axes\n",
    "    break  # Display one batch only\n",
    "plt.show()\n",
    "\n",
    "# Model Building\n",
    "# Define a function to create a Convolutional Neural Network (CNN) model\n",
    "def create_model(input_shape=(img_height, img_width, 3), num_classes=9):\n",
    "    model = models.Sequential()  # Start a sequential model\n",
    "    \n",
    "    # First Convolutional Block (32 filters, 3x3 filter size, ReLU activation, followed by Max Pooling)\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))  # Reduce size using max pooling\n",
    "    \n",
    "    # Second Convolutional Block (64 filters, 3x3 filter size)\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))  # Another max pooling layer\n",
    "    \n",
    "    # Third Convolutional Block (128 filters, 3x3 filter size)\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))  # Max pooling again\n",
    "    \n",
    "    # Fully Connected Layers\n",
    "    model.add(layers.Flatten())  # Flatten the output (convert 2D matrix to 1D)\n",
    "    model.add(layers.Dense(128, activation='relu'))  # Fully connected dense layer with 128 units\n",
    "    model.add(layers.Dropout(0.5))  # Dropout to prevent overfitting (50% of nodes dropped)\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))  # Output layer for multiclass classification (softmax gives probabilities)\n",
    "\n",
    "    return model  # Return the created model\n",
    "\n",
    "# Model Compilation\n",
    "cnn_model = create_model()  # Create the CNN model\n",
    "\n",
    "# Compile the model with Adam optimizer, sparse categorical crossentropy (for integer-encoded labels), and accuracy metric\n",
    "cnn_model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Dynamic Augmentation for Under-represented Classes using Augmentor\n",
    "parent_directory = \"/content/drive/MyDrive/Cancer_Dataset/train\"\n",
    "\n",
    "# List of all 9 classes\n",
    "classes = [\n",
    "    'actinic keratosis', \n",
    "    'basal cell carcinoma', \n",
    "    'dermatofibroma', \n",
    "    'melanoma', \n",
    "    'nevus', \n",
    "    'pigmented benign keratosis', \n",
    "    'seborrheic keratosis', \n",
    "    'squamous cell carcinoma', \n",
    "    'vascular lesion'\n",
    "]\n",
    "\n",
    "# Apply augmentation for each class\n",
    "for class_name in classes:\n",
    "    class_path = os.path.join(parent_directory, class_name)\n",
    "    \n",
    "    # Check if the directory exists\n",
    "    if os.path.exists(class_path):\n",
    "        print(f\"Processing class: {class_path}\")\n",
    "        \n",
    "        # Apply augmentation using Augmentor\n",
    "        p = Augmentor.Pipeline(class_path)\n",
    "        p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n",
    "        p.zoom_random(probability=0.5, percentage_area=0.8)\n",
    "        p.flip_left_right(probability=0.5)\n",
    "        p.sample(1000)  # Generate 1000 images for each class\n",
    "    else:\n",
    "        print(f\"Directory does NOT exist: {class_path}\")\n",
    "\n",
    "# Model Training\n",
    "epochs = 20  # Number of times to go through the entire dataset\n",
    "history = cnn_model.fit(\n",
    "    train_generator,  # Training data\n",
    "    validation_data=validation_generator,  # Validation data\n",
    "    epochs=epochs  # Number of epochs\n",
    ")\n",
    "\n",
    "# Evaluate Overfitting or Underfitting\n",
    "# Plotting training and validation accuracy and loss to see if the model is overfitting/underfitting\n",
    "acc = history.history['accuracy']  # Training accuracy\n",
    "val_acc = history.history['val_accuracy']  # Validation accuracy\n",
    "\n",
    "loss = history.history['loss']  # Training loss\n",
    "val_loss = history.history['val_loss']  # Validation loss\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')  # Plot training accuracy\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')  # Plot validation accuracy\n",
    "plt.legend(loc='lower right')  # Add a legend\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')  # Plot training loss\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')  # Plot validation loss\n",
    "plt.legend(loc='upper right')  # Add a legend\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "# Data Augmentation for Overfitting\n",
    "# Augmenting data (rotation, shifting, zooming, flipping) to prevent overfitting\n",
    "train_datagen_augmented = ImageDataGenerator(\n",
    "    rescale=1./255,  # Rescale pixel values between 0 and 1\n",
    "    rotation_range=40,  # Randomly rotate images by 40 degrees\n",
    "    width_shift_range=0.2,  # Randomly shift images horizontally\n",
    "    height_shift_range=0.2,  # Randomly shift images vertically\n",
    "    shear_range=0.2,  # Shear the images\n",
    "    zoom_range=0.2,  # Randomly zoom in on images\n",
    "    horizontal_flip=True,  # Randomly flip images horizontally\n",
    "    fill_mode='nearest',  # Fill in missing pixels after transformations\n",
    "    validation_split=0.2  # Reserve 20% for validation\n",
    ")\n",
    "\n",
    "# Creating augmented training data\n",
    "train_generator_augmented = train_datagen_augmented.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse',\n",
    "    subset='training'  # Only for training\n",
    ")\n",
    "\n",
    "# Training the model on augmented data\n",
    "history_augmented = cnn_model.fit(\n",
    "    train_generator_augmented,  # Augmented training data\n",
    "    validation_data=validation_generator,  # Validation data\n",
    "    epochs=20  # Number of epochs\n",
    ")\n",
    "\n",
    "# Step 4: Train the model on rectified class imbalance data for 30 epochs\n",
    "# Reload the balanced data using ImageDataGenerator\n",
    "train_datagen_balanced = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_generator_balanced = train_datagen_balanced.flow_from_directory(\n",
    "    train_dir,  # The directory with rectified (augmented) data\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Train the model on the rectified dataset for 30 epochs\n",
    "history_balanced = cnn_model.fit(\n",
    "    train_generator_balanced,  # Balanced training data\n",
    "    validation_data=validation_generator,  # Validation data\n",
    "    epochs=30  # Train for 30 epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b42cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
